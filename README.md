# LangChain RAG with Hugging Face (FLAN-T5)

This project demonstrates a **Retrieval-Augmented Generation (RAG)** pipeline using **LangChain**, **Hugging Face Transformers**, and **FAISS**, running fully **locally** (no OpenAI API required).

It answers user questions by retrieving relevant information from a local text file and generating grounded responses using a Hugging Face LLM.

---

## âœ¨ Features

- Local LLM using **google/flan-t5-base**
- Document-based question answering (RAG)
- Vector search with **FAISS**
- Sentence embeddings using **Sentence Transformers**
- Optional web search tool via **Tavily**
- Clean project structure, GitHub-ready

---

## ğŸ§  How It Works (RAG Flow)

1. Load documents from `sample.txt`
2. Convert text into embeddings
3. Store embeddings in FAISS vector database
4. Retrieve relevant chunks for a query
5. Generate a grounded answer using the LLM

---

## ğŸ“ Project Structure

```text
langchain_handson/
â”‚
â”œâ”€â”€ main.py                 # Main application
â”œâ”€â”€ sample.txt              # Knowledge source for RAG
â”œâ”€â”€ requirements.txt        # Clean dependency list
â”œâ”€â”€ requirements-lock.txt   # Exact environment snapshot
â”œâ”€â”€ .gitignore              # Ignored files (env, venv, cache)
â””â”€â”€ README.md               # Project documentation

```
